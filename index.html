<!doctype html>
<html lang="en">
    <head>
        <title>Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces</title>
        <link rel="icon" type="image/x-icon" href="/static/img/icons/brain.ico">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://VSI-Bench.github.io/" />
        <meta property="og:image" content="static/img/preview.png" />
        <meta property="og:title" content="Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces" />
        <!-- <meta property="og:description" content="Cambrian-1 is a family of multimodal LLMs with a vision-centric design. We also release CV-Bench, a new vision-centric benchmark, and Cambrian-10M, a multimodal instruction-tuning dataset." /> -->
        
        <!-- Twitter -->
        <meta name="twitter:url" content="https://VSI-Bench.github.io/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="static/img/preview.png" />
        <meta name="twitter:title" content="Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces" />
        <meta name="twitter:description" content="We introduce VSI-Bench, a novel benchmark of over 5,000 video-based visual-spatial intelligence questions, to evaluate and probe MLLMs, which revealed that their emerging spatial reasoning and local world modeling capabilities remain subhuman but promising.        " />

        <script src="/static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="/static/js/hider.js"></script>
        <script src="/static/js/image_interact.js"></script>
        <script src="/static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="/static/css/style.css">
        <link rel="stylesheet" href="/static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="/static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="/static/js/medium-zoom.min.js"></script>
        <script defer src="/static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Thinking in Space</i></h1>
                    <h2>How Multimodal Large Language Models 
                        See, Remember and Recall Spaces</h2>

                        <div class="icon-container">
                            <div class="icon-item">
                                
                                <img src="/static/img/icons/bench.svg" alt="Visual Representation Icon">
                                <div><strong>VSI-Bench</strong>: We introduce a high-quality benchmark for the evaluation of 3D, video-based, visual-spatial intelligence of MLLMs.</div>
                            </div>
                            <div class="icon-item">
                                <img src="/static/img/icons/evaluation.svg" alt="Connector Design Icon">
                                <div><strong>Evaluation Protocol</strong>: We attribute VSI-Bench performance to spatial cognitive capabilities, shedding light on areas of improvement.</div>
                            </div>
                            <div class="icon-item">
                                <img src="/static/img/icons/world.svg" alt="Instruction Tuning Data Icon">
                                <div><strong>World Models</strong>: We analyze the implicit linguistic and visual world models.</div>
                            </div>
                            <div class="icon-item">
                                <img src="/static/img/icons/improvement.svg" alt="Instruction Tuning Recipes Icon">
                                <div><strong>Improvements</strong>: We conduct preliminary exploration of methods for improvement.</div>
                            </div>
                        </div>
                    

                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>PDF</span>
                        </a>
                        <!-- replace image -->
                        <a href="" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>                      
                        <a href="" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>VSI-Bench</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img draggable="false" src="static/img/preview.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>

        
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column" style="line-height: 1.2;">
                    <h3 style="font-size: 1.2em; margin-bottom: 0.2em;">Authors</h3>
                    <p style="margin: 0;">
                        <a href="https://jihanyang.github.io/" class="author-link" target="_blank" style="font-size: 0.9em;">Jihan Yang</a> 
                        <sup style="font-size: 0.9em;">△*</sup>
                    </p>
                    <p style="margin: 0;">
                        <a href="https://github.com/vealocia" class="author-link" target="_blank" style="font-size: 0.9em;">Shusheng Yang</a> 
                        <sup style="font-size: 0.9em;">△*</sup>
                    </p>
                    <p style="margin: 0;">
                        <a href="https://anjaliwgupta.com" class="author-link" target="_blank" style="font-size: 0.9em;">Anjali W. Gupta</a> 
                        <sup style="font-size: 0.9em;">△*</sup>
                    </p>
                    <p style="margin: 0;">
                        <a href="https://rilynhan.github.io" class="author-link" target="_blank" style="font-size: 0.9em;">Rilyn Han</a> 
                        <sup style="font-size: 0.9em;">▲*</sup>
                    </p>
                    <p style="margin: 0;">
                        <a href="https://profiles.stanford.edu/fei-fei-li" class="author-link" target="_blank" style="font-size: 0.9em;">Li Fei-Fei</a> 
                        <sup style="font-size: 0.9em;">★</sup>
                    </p>
                    <p style="margin: 0;">
                        <a href="https://www.sainingxie.com/" class="author-link" target="_blank" style="font-size: 0.9em;">Saining Xie</a> 
                        <sup style="font-size: 0.9em;">△</sup>
                    </p>
                </div>
                
                <div class="byline-column" style="line-height: 1.2;">
                    <h3 style="font-size: 1.2em; margin-bottom: 0.2em;">Affiliations</h3>
                    <p style="margin: 0;">
                        <sup style="font-size: 0.8em;">△</sup>
                        <a href="https://cs.nyu.edu/home/index.html" class="affiliation-link" target="_blank" style="font-size: 0.9em;">New York University</a>
                    </p>
                    <p style="margin: 0;">
                        <sup style="font-size: 0.8em;">▲</sup>
                        <a href="https://www.yale.edu" class="affiliation-link" target="_blank" style="font-size: 0.9em;">Yale University</a>
                    </p>
                    <p style="margin: 0;">
                        <sup style="font-size: 0.8em;">★</sup>
                        <a href="https://www.stanford.edu/" class="affiliation-link" target="_blank" style="font-size: 0.9em;">Stanford University</a>
                    </p>
                </div>
                
                <div class="byline-column" style="line-height: 1.2;">
                    <h3 style="font-size: 1.2em; margin-bottom: 0.2em;">Date</h3>
                    <p style="margin: 0; font-size: 0.9em;">
                        Dec. 17<sup style="font-size: 0.8em;">th</sup>, 2024
                    </p>
                </div>
                
            </div>
        </div>
    <div class = "new-class-author">

        <p style="text-align: center;">
            <span class="author-note"><sup>*</sup>Equal contribution</span>&emsp;
            <!-- <span class="author-note"><sup>†</sup>Project lead</span>&emsp; -->
            <!-- <span class="author-note"><sup>†</sup>Corresponding author</span> -->
        </p>
    </div>
        


        <!-- <div id="slider-video-tourist" class="slider-video-container">
            <div class="my-slides">
                <p class="video-question">Question 1: What do you think is the main idea of this video?</p>
                <video class="video-slide" controls preload="metadata" playsinline>
                    <source src="static/video/intentional_explorer.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <textarea class="answer-box" placeholder="Type your answer here..."></textarea>
            </div>
        
            <div class="my-slides">
                <p class="video-question">Question 2: What message does this collaboration convey?</p>
                <video class="video-slide" controls preload="metadata" playsinline>
                    <source src="static/video/rx-399_hk.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <textarea class="answer-box" placeholder="Type your answer here..."></textarea>
            </div>
        
            <div class="my-slides">
                <p class="video-question">Question 3: How does this video portray cultural exchange?</p>
                <video class="video-slide" controls preload="metadata" playsinline>
                    <source src="static/video/intentional_explorer.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <textarea class="answer-box" placeholder="Type your answer here..."></textarea>
            </div>
        
            <div class="my-slides">
                <p class="video-question">Question 4: What elements make this video stand out?</p>
                <video class="video-slide" controls preload="metadata" playsinline>
                    <source src="static/video/intentional_explorer.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <textarea class="answer-box" placeholder="Type your answer here..."></textarea>
            </div>
        
            <a class="prev" onclick="plusSlides('slider-video-tourist', -1)">❮</a>
            <a class="next" onclick="plusSlides('slider-video-tourist', 1)">❯</a>
            
        </div>
         -->

         <p style="text-align: center; font-weight: bold; margin-top: 20px;">
            Compare your spatial intelligence abilities with Gemini!
        </p>

         <div class="l-body">
            <!-- Preview Images in a Flex Container - placed below the Q&A sections -->
            <div class="preview-container">
                <img id="Touristvideo1Preview" class="preview" src="static/img/video-preview/41069025-thumbnail.jpg" alt="video-thumbnail0" onclick="switchVideo('Tourist', 'video1Container', 'video1Preview')">
                <img id="Touristvideo2Preview" class="preview" src="static/img/video-preview/41125700-thumbnail.jpg" alt="video-thumbnail3" onclick="switchVideo('Tourist','video2Container', 'video2Preview')">
                <img id="Touristvideo3Preview" class="preview" src="static/img/video-preview/42445981-thumbnail.jpg" alt="video-thumbnail18" onclick="switchVideo('Tourist', 'video3Container', 'video3Preview')">
                <img id="Touristvideo4Preview" class="preview" src="static/img/video-preview/42446049-thumbnail.jpg" alt="video-thumbnail1129" onclick="switchVideo('Tourist','video4Container', 'video4Preview')">
                <img id="Touristvideo5Preview" class="preview" src="static/img/video-preview/41069025-thumbnail.jpg" alt="video-thumbnail1236" onclick="switchVideo('Tourist', 'video5Container', 'video5Preview')">
                <img id="Touristvideo6Preview" class="preview" src="static/img/video-preview/47334096-thumbnail.jpg" alt="video-thumbnail1462" onclick="switchVideo('Tourist','video6Container', 'video6Preview')">
                <img id="Touristvideo7Preview" class="preview" src="static/img/video-preview/47332899-thumbnail.jpg" alt="video-thumbnail879" onclick="switchVideo('Tourist', 'video7Container', 'video7Preview')">
                <img id="Touristvideo8Preview" class="preview" src="static/img/video-preview/09c1414f1b-thumbnail.jpg" alt="video-thumbnail2871" onclick="switchVideo('Tourist','video8Container', 'video8Preview')">
            </div>
            <!-- Video 1; ID: 0 -->
            <div id="Touristvideo1Container" class="video-container">
                <div class="video-label">Object Count, 1/3</div>
                <video class="video-music" controls preload="metadata" playsinline>
                    <source src="static/videos/41069025.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                
                <div class="video-qa">
                    <div class="qa-item">
                        <p><strong>Question:</strong> How many table(s) are in this room?</p>
                        <p><strong>Options:</strong></p>
                        <ul style="list-style-type: none; padding-left: 0; display: flex; justify-content: space-between; text-align: left; max-width: 60%; margin: 0 auto;">
                            <li><input type="radio" name="tables"> A. 7</li>
                            <li><input type="radio" name="tables"> B. 4</li>
                            <li><input type="radio" name="tables"> C. 1</li>
                            <li><input type="radio" name="tables"> D. 6</li>
                        </ul>  
                                           
                    
            
                    <div id="answer-1" style="display:none; text-align: center;">
                        <div style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
                            <!-- GT aligned with A and C -->
                            <p>
                                <strong style="color: green;">Ground Truth:</strong> B. 4
                            </p>
                            <!-- Gemini aligned with B and D -->
                            <p>
                                <strong style="color: red;">Gemini:</strong> 2
                            </p>
                        </div>
                    </div>
                    
            
                    <div style="margin-top: 20px; text-align: center;"> <!-- Increased margin-top -->
                        <p class="click-hint" style="width:85%; cursor:pointer;" onclick="toggleAnswer('answer-1', this)">
                            <img src="static/img/icons/teaser.gif" style="width:1.5rem">
                            <strong>Click to view Ground Truth and the answer from Gemini!</strong>
                        </p>
                    </div>
                </div>
                </div>
            </div>
        
            <!-- Video 2; ID: 3 -->
            <div id="Touristvideo2Container" class="video-container" style="display:none;">
                <div class="video-label">Object Count, 2/3</div>
                <video class="video-music" controls preload="metadata" playsinline>
                    <source src="static/videos/41125700.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                
                <div class="video-qa">
                    <div class="qa-item">
                                <p><strong>Question:</strong> How many sofa(s) are in this room?</p>
                                <p><strong>Options:</strong></p>
                                <ul style="list-style-type: none; padding-left: 0; display: flex; justify-content: space-between; text-align: left; max-width: 60%; margin: 0 auto;">
                                    <li><input type="radio" name="tables"> A. 4</li>
                                    <li><input type="radio" name="tables"> B. 5</li>
                                    <li><input type="radio" name="tables"> C. 2</li>
                                    <li><input type="radio" name="tables"> D. 1</li>
                                </ul>  
                        
                                <div id="answer-2" style="display:none; text-align: center; margin-top: 20px;">
                                    <div style="display: flex; justify-content: center; gap: 20px;">
                                        <!-- GT aligned with A and C -->
                                        <p>
                                            <strong style="color: green;">Ground Truth:</strong> C. 2
                                        </p>
                                        <!-- Gemini aligned with B and D -->
                                        <p>
                                            <strong style="color: red;">Gemini:</strong> D. 1
                                        </p>
                                    </div>
                                </div>
                        
                                <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-2', this)">
                                    <img src="static/img/icons/teaser.gif" style="width:1.5rem">
                                    <strong>Click to view Ground Truth and the answer from Gemini!</strong>
                                </p>
                            </div>
                        </div>
                    </div>    

            <!-- Video 3; ID: 18 -->
            <div id="Touristvideo3Container" class="video-container" style="display:none;">
                <div class="video-label">Object Count, 3/3</div>
                <video class="video-music" controls preload="metadata" playsinline>
                    <source src="static/videos/42445981.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                
                <div class="video-qa">
                    <div class="qa-item">
                        <p><strong>Question:</strong> How many chair(s) are in this room?</p>
                        <p><strong>Options:</strong></p>
                        <ul style="list-style-type: none; padding-left: 0; display: flex; justify-content: space-between; text-align: left; max-width: 60%; margin: 0 auto;">
                            <li><input type="radio" name="tables"> A. 5</li>
                            <li><input type="radio" name="tables"> B. 8</li>
                            <li><input type="radio" name="tables"> C. 4</li>
                            <li><input type="radio" name="tables"> D. 3</li>
                        </ul>  
                
                        <div id="answer-3" style="display:none; text-align: center; margin-top: 20px;">
                            <div style="display: flex; justify-content: center; gap: 20px;">                            <p>
                                <strong style="color: green;">Ground Truth:</strong> A. 5
                            </p>
                            <p>
                                <strong style="color: red;">Gemini:</strong> D. 3
                            </p>
                        </div>
                    </div>
                
                        <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-3', this)">
                            <img src="static/img/icons/teaser.gif" style="width:1.5rem">
                            <strong>Click to view Ground Truth and the answer from Gemini!</strong>
                        </p>
                    </div>
                
            </div>
            </div>


            <!-- Video 4; ID: 1129 -->
            <div id="Touristvideo4Container" class="video-container" style="display:none;">
                <div class="video-label">Relative Direction, 1/2</div>
                <video class="video-music" controls preload="metadata" playsinline>
                    <source src="static/videos/42446049.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                
                <div class="video-qa">
                    <div class="qa-item">
                        <p><strong>Question:</strong> If I am standing by the refrigerator and facing the washer, is the stove to my left, right, or back? 
                            An object is to my back if I would have to turn at least 135 degrees in order to face it.
                        </p>
                        <p><strong>Options:</strong></p>
                        <ul style="list-style-type: none; padding-left: 0; display: flex; justify-content: space-evenly; text-align: left; max-width: 60%; margin: 0 auto;">
                            <li><input type="radio" name="tables"> A. Back</li>
                            <li><input type="radio" name="tables"> B. Right</li>
                            <li><input type="radio" name="tables"> C. Left</li>
                        </ul>  
                
                        <div id="answer-4" style="display:none; text-align: center; margin-top: 20px;">
                            <div style="display: flex; justify-content: center; gap: 20px;">    
                            <p>
                                <strong style="color: green;">Ground Truth:</strong> B. Right
                            </p>
                            <p>
                                <strong style="color: red;">Gemini:</strong> C. Left
                            </p>
                        </div>
                    </div>
                
                        <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-4', this)">
                            <img src="static/img/icons/teaser.gif" style="width:1.5rem">
                            <strong>Click to view Ground Truth and the answer from Gemini!</strong>
                        </p>
                    </div>
                </div>
                
            </div>

            <!-- Video 5; ID: 1236 -->
            <div id="Touristvideo5Container" class="video-container" style="display:none;">
                <div class="video-label">Relative Direction, 2/2</div>

                <video class="video-music" controls preload="metadata" playsinline>
                    <source src="static/videos/41069025.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                

                        <div class="video-qa">
                            <div class="qa-item">
                                <p><strong>Question:</strong> If I am standing by the stove and facing the sofa, is the TV to the left or the right of the sofa?</p>
                                <p><strong>Options:</strong></p>
                                <ul style="list-style-type: none; padding-left: 0; display: flex; justify-content: center; text-align: left; max-width: 60%; margin: 0 auto;">
                                    <li style="margin-right: 30px;"><input type="radio" name="tables"> A. Left</li>
                                    <li><input type="radio" name="tables"> B. Right</li>
                                </ul>  
                        
                                <div id="answer-5" style="display:none; text-align: center; margin-top: 20px;">
                                    <div style="display: flex; justify-content: center; gap: 20px;">    
                                    <p>
                                        <strong style="color: green;">Ground Truth:</strong> A. Left
                                    </p>
                                    <p>
                                        <strong style="color: red;">Gemini:</strong> B. Right
                                    </p>
                                </div>
                            </div>
                                <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-5', this)">
                                    <img src="static/img/icons/teaser.gif" style="width:1.5rem">
                                    <strong>Click to view Ground Truth and the answer from Gemini!</strong>
                                </p>
                            </div>
                        </div>
            </div>

            <!-- Video 6; ID: 1462 -->
            <div id="Touristvideo6Container" class="video-container" style="display:none;">
                <div class="video-label">Relative Distance, 1/1</div>

                <video class="video-music" controls preload="metadata" playsinline>
                    <source src="static/videos/47334096.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                
                <div class="video-qa">
                    <div class="qa-item">
                        <p><strong>Question:</strong> Measuring from the closest point of each object, which of these objects (table, stool, sofa, stove) is the closest to the TV?</p>
                        <p><strong>Options:</strong></p>
                        <ul style="list-style-type: none; padding-left: 0; display: flex; justify-content: space-between; text-align: left; max-width: 60%; margin: 0 auto;">
                            <li><input type="radio" name="tables"> A. Table</li>
                            <li><input type="radio" name="tables"> B. Stool</li>
                            <li><input type="radio" name="tables"> C. Sofa</li>
                            <li><input type="radio" name="tables"> D. Stove</li>
                        </ul>  
                
                        <div id="answer-6" style="display:none; text-align: center; margin-top: 20px;">
                            <div style="display: flex; justify-content: center; gap: 20px;">
                            <p>
                                <strong style="color: green;">Ground Truth:</strong> A. Table
                            </p>
                            <p>
                                <strong style="color: red;">Gemini:</strong> C. Sofa
                            </p>
                        </div>
                    </div>
                        <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-6', this)">
                            <img src="static/img/icons/teaser.gif" style="width:1.5rem">
                            <strong>Click to view Ground Truth and the answer from Gemini!</strong>
                        </p>
                    </div>
                </div>
                
            </div>


            <!-- Video 7; ID: 879 -->
            <div id="Touristvideo7Container" class="video-container" style="display:none;">
                <div class="video-label">Absolute Distance, 1/1</div>

                <video class="video-music" controls preload="metadata" playsinline>
                    <source src="static/videos/47332899.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                
                <div class="video-qa">
                    <div class="qa-item">
                        <p><strong>Question:</strong> Measuring from the closest point of each object, what is the distance between the stool and the bed (in meters)?</p>
                        <p><strong>Options:</strong></p>
                        <ul style="list-style-type: none; padding-left: 0; display: flex; justify-content: space-between; text-align: left; max-width: 60%; margin: 0 auto;">
                            <li><input type="radio" name="tables"> A. 0.7</li>
                            <li><input type="radio" name="tables"> B. 0.3</li>
                            <li><input type="radio" name="tables"> C. 1.2</li>
                            <li><input type="radio" name="tables"> D. 0.9</li>
                        </ul>  
                
                        <div id="answer-7" style="display:none; text-align: center; margin-top: 20px;">
                            <div style="display: flex; justify-content: center; gap: 20px;">
                            <p>
                                <strong style="color: green;">Ground Truth:</strong> A. 0.7
                            </p>
                            <p>
                                <strong style="color: red;">Gemini:</strong> 2.5
                            </p>
                        </div>
                    </div>
                        <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-7', this)">
                            <img src="static/img/icons/teaser.gif" style="width:1.5rem">
                            <strong>Click to view Ground Truth and the answer from Gemini!</strong>
                        </p>
                    </div>
                </div>
                
            </div>     
            
            
            <!-- Video 8; ID: 2871 -->
            <div id="Touristvideo8Container" class="video-container" style="display:none;">
                <div class="video-label">Appearance Order, 1/1</div>

                <video class="video-music" controls preload="metadata" playsinline>
                    <source src="static/videos/09c1414f1b.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                
                <div class="video-qa">
                    <div class="qa-item">
                        <p><strong>Question:</strong> What will be the first-time appearance order of the following categories in the video: blanket, trash can, microwave, plant?</p>
                        <p><strong>Options:</strong></p>
                        <div style="display: flex; justify-content: center; flex-wrap: wrap; max-width: 800px; margin: 0 auto; gap: 20px;">
                            <div style="flex: 1; min-width: 300px; text-align: left;">
                                <input type="radio" id="optionA" name="appearanceOrder"> 
                                <label for="optionA">A. microwave, blanket, plant, trash can</label>
                            </div>
                            <div style="flex: 1; min-width: 300px; text-align: left;">
                                <input type="radio" id="optionB" name="appearanceOrder">
                                <label for="optionB">B. plant, blanket, microwave, trash can</label>
                            </div>
                            <div style="flex: 1; min-width: 300px; text-align: left;">
                                <input type="radio" id="optionC" name="appearanceOrder">
                                <label for="optionC">C. plant, blanket, trash can, microwave</label>
                            </div>
                            <div style="flex: 1; min-width: 300px; text-align: left;">
                                <input type="radio" id="optionD" name="appearanceOrder">
                                <label for="optionD">D. blanket, trash can, microwave, plant</label>
                            </div>
                        </div>
                        
            </div>

        </div>
                
        <div id="answer-8" style="display:none; text-align: center; margin-top: 20px;">
            <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p>
                    <strong style="color: green;">Ground Truth:</strong> C. plant, blanket, trash can, microwave.
                </p>
                <p>
                    <strong style="color: red;">Gemini:</strong> B. plant, blanket, microwave, trash can.
                </p>
            </div>
        </div>
        
                        <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-8', this)">
                            <img src="static/img/icons/teaser.gif" style="width:1.5rem">
                            <strong>Click to view Ground Truth and the answer from Gemini!</strong>
                        </p>
                    </div>
                </div>                
            </div>              
            <script>
            function toggleAnswer(answerId, element) {
                const answerElement = document.getElementById(answerId);
                const currentlyHidden = answerElement.style.display === "none";
            
                if (currentlyHidden) {
                    // Show the answer
                    answerElement.style.display = "block";
                    element.innerHTML = '<img src="static/img/icons/teaser.gif" style="width:1.5rem"> <strong>Hide answer</strong>';
                } else {
                    // Hide the answer
                    answerElement.style.display = "none";
                    element.innerHTML = '<img src="static/img/icons/teaser.gif" style="width:1.5rem"> <strong>Click to view Ground Truth and the answer from Gemini!</strong>';
                }
            }
            </script>
        

        </div>
        
        


        
        
        

        <d-figure id="fig-teaser">
            <figure>
                <img data-zoomable="" draggable="false" src="static/img/teaser.png" alt="Visual-Spatial Intelligence Teaser">
                <figcaption>
                    <strong>Figure 1:</strong> Can Multimodal LLMs “think spatially” when presented with a video recording of an environment? Can they build an accurate, implicit “cognitive map” that allows them to answer questions about a space? What are the strengths and limitations of using MLLMs to enhance spatial intelligence? We dig into these questions by setting up video data for MLLMs to watch, building a VQA benchmark to check their recall, and examining what the MLLMs actually remember and understand.
                </figcaption>
            </figure>
        </d-figure>

    
        <p class="text abstract">
            We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive - though subhuman - visual-spatial intelligence. 
            Our evaluation reveals that MLLMs exhibit competitive visual-spatial intelligence, if still well short of human-level. 
            To understand the MLLMs' behavior, we probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models.
            <br>
        </p>


        <div class="icon-row">
            <a href="#vsi-benchmark" class="icon-link">
                <img src="/static/img/icons/bench.svg" alt="Visual Representation Logo" class="icon">
                VSI-Bench<br>Construction
            </a>
            <a href="#vsi-evaluation" class="icon-link">
                <img src="/static/img/icons/evaluation.svg" alt="Evaluation Logo" class="icon">
                Evaluation<br>Protocol
            </a>
            <a href="#visual-representation" class="icon-link">
                <img src="/static/img/icons/speech.svg" alt="Speech Logo" class="icon">
                Linguistical<br>Analysis
            </a>
            <a href="#vsi-improvements" class="icon-link">
                <img src="/static/img/icons/vision.svg" alt="Vision Logo" class="icon">
                Visual<br>Analysis
            </a>

        </div>

        <p class="click-hint2" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>
        

        <hr>

        <div id='vsi-benchmark' class="vsi-benchmark">
            
            <div id="sec:vsi-overview" class="sub-section">
                <h1 class="text">VSI-Bench</h1>

                    <p class="text">
                        <p class="text">
                        <strong>Benchmark Overview:</strong> We develop VSI-Bench, a benchmark to evaluate the visual-spatial intelligence of Multimodal LLMs (MLLMs) using over 5,000 question-answer pairs derived from 288 egocentric videos sourced from the validation sets of public indoor 3D scene reconstruction datasets ScanNet, ScanNet++, and ARKitScenes. VSI-Bench includes eight tasks under three task types: configurational, measurement estimation, and spatiotemporal. See Fig. 2 for an overview of the tasks in VSI-Bench and Fig. 3 for dataset statistics. Iteratively refined for quality, VSI-Bench provides a foundation to study the connection between MLLMs and 3D reconstruction.</p>
                    <d-figure id="fig-task-demo" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/task-demo.png" alt="benchmark category">
                            <figcaption>
                                <strong>Figure 2: Tasks demonstration of VSI-Bench.</strong> 
                                Note: the questions above are simplified slightly for clarity and brevity.
                            </figcaption>
                        </figure>
                    </d-figure>
                    <d-figure id="fig-bench-stats">
                        <figure>
                            <!-- <div style="display: flex; align-items: center; gap: 0px; margin-left: -250px">
                                <div style="flex: 3; display: flex; justify-content: center; align-items: center; overflow: hidden; position: relative; margin: 0;">
                                    <iframe 
                                        src="static/img/dataset_overview_donut_chart.html" 
                                        style="width: 100%; height: 450px; border: none; transform: scale(0.7); transform-origin: center; padding: 0; margin: 0;" 
                                        title="Dataset Overview Donut Chart">
                                    </iframe>
                                </div>
                     -->
                                <!-- Display the image -->
                                <!-- <div style="flex: 1; height: 500px; margin-left: -300px;">
                                    <img 
                                        data-zoomable="" 
                                        draggable="false" 
                                        src="static/img/video-length.png" 
                                        alt="Fit Visualization" 
                                        style="width: 100%; height: 100%; object-fit: contain; margin: 0px; padding: 0;">
                                </div>
                            </div> -->
                            <img data-zoomable="" draggable="false" src="static/img/benchmark-stats.png" alt="benchmark category" style="width: 90%; height: auto; display: block; margin: 0 auto;">


            
                            <figcaption style="text-align: center; margin-top: 20px;">
                                <strong>Figure 3: Benchmark Statistics.</strong> 
                                <strong>Left</strong>: The distribution of tasks across three main categories. <strong>Right</strong>: The video length statistic.
                            </figcaption>
                        </figure>
                    </d-figure>
                    
            </div>
            <div id="vsi-construct" class="sub-section">
                

                    <p class="text"><strong>VSI-Bench Construction:</strong>
                        We develop a robust pipeline to construct VSI-Bench that enables high-quality question-answer (QA) pair generation at scale. Starting with data collection and unification, we standardize diverse 3D indoor scene datasets into a unified meta-information format, incorporating object categories, bounding boxes, and video specifications to support dataset-agnostic QA generation. QA pairs are generated using automated annotations from meta-information and task-specific question templates, with route planning tasks manually annotated. To ensure quality, we implement a human-in-the-loop review process, iteratively refining question templates, annotations, and QA generation rules by addressing ambiguities and errors flagged by evaluators. </p>

                    <d-figure id="fig-bench-pipeline" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/benchmark_construction.png" alt="benchmark category">
                            <figcaption>
                                <strong>Figure 4: Benchmark curation pipeline.</strong> The pipeline unifies datasets into a standardized format and semantic space for consistent processing. QA pairs are then generated through both human annotation and question templates. To ensure quality, human verification is implemented at all key stages for filtering low-quality videos, annotations, and ambiguous QA pairs.
                            </figcaption>
                        </figure>
                    </d-figure>

            </div>
        <div id="vsi-evaluation" class="vsi-evaluation">
            <h1 class="text">Evaluation on VSI-Bench</h1>

                    <!-- <p class="text">
                        <p class="text">
                        We benchmarked 15 video-supporting MLLMs from diverse model families. For proprietary models, we consider Gemini-1.5 and GPT-4o. For open-source models, we evaluate models from InternVL2, ViLA, 203 LongViLA, LongVA, LLaVA-OneVision, and LLaVA-NeXT-Video. All evaluations were conducted in zero-shot settings with default prompts and greedy decoding for reproducibility. Tasks were evaluated using either Multiple-Choice Answer (MCA) accuracy or our proposed Mean Relative Accuracy (MRA) for Numerical Answer (NA) tasks. Baselines include random selection and frequency-based answer selection to identify performance gains due to distribution biases. Additionally, human performance was assessed on a randomly sampled subset of 400 questions (VSI-Bench tiny), with metrics compared to Gemini-1.5 Pro.
                    </p> -->
                    <script type="text/javascript" id="MathJax-script" async
                    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
                    </script>
            <p class="text"><strong>Evaluation Setup</strong>:
                We benchmarked 15 video-supporting MLLMs from diverse model families. For proprietary models, we consider Gemini-1.5 and GPT-4o. For open-source models, we evaluate models from InternVL2, ViLA, LongViLA, LongVA, LLaVA-OneVision, and LLaVA-NeXT-Video. All evaluations are conducted in zero-shot settings with default prompts and greedy decoding for reproducibility. Tasks are evaluated using either Multiple-Choice Answer (MCA) accuracy or our proposed Mean Relative Accuracy (MRA) for Numerical Answer (NA) tasks.
            </p>
        
            <div class="formula">
                $$\text{MRA} = \frac{1}{10} \sum_{\theta \in C} \mathbb{1}\left(\frac{| \hat{y} - y |}{y} < 1 - \theta\right)$$
            </div>
        
            <p class="text">
                Baselines include random selection and frequency-based answer selection to identify performance gains due to distribution biases. Additionally, human performance is assessed on a randomly sampled subset of 400 questions (VSI-Bench tiny), with metrics compared to Gemini-1.5 Pro.
            </p>
                   
            
            <p class="text"><strong>Main Results</strong>:
                Human evaluators achieve an average accuracy of 79%, outperforming the best model by 33%, with near-perfect performance (94%-100%) on configuration and spatiotemporal tasks. However, the gap narrows on measurement tasks that require precise estimation, where MLLMs demonstrate relative strength in quantitative tasks. Among proprietary models, Gemini-1.5 Pro stands out, significantly exceeding chance baselines and approaching human performance in tasks like absolute distance and room size estimation, despite being trained only on 2D digital data. Top-performing open-source models, such as LLaVA-NeXT-Video-72B and LLaVA-OneVision-72B, achieve competitive results, trailing Gemini-1.5 Pro by just 4%-5%. However, most open-source models (7/12) fall below chance baselines, revealing notable deficiencies in visual-spatial intelligence.
            </p>
            <d-figure id="fig-eval-bench" >
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/eval-bench.png" alt="benchmark category">
                    <figcaption>
                        <strong>Table 1:  Evaluation on VSI-Bench.</strong> <strong>Left</strong>: Dark gray indicates the best result among all models and light gray indicates the best
                        result among open-source models. † indicates results on VSI-Bench (tiny) set. <strong>Right</strong>: Results including the top-3 open-source models.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text"><strong>Blind Evaluation</strong>:
                We compare MLLMs’ performance against “Chance Level (frequency)” and “Vision Disabled” (blind) results, averaged across six top models (three open-source and three closed-source). The consistent improvements in “Enabled−Disabled” and the general degradation in “Disabled−Chance” highlight the importance of video input for VSI-Bench, as blind models perform worse than chance. However, MLLMs struggle to surpass chance level on tasks such as absolute distance estimation, route planning, and relative direction, reflecting the inherent difficulty of these tasks. Interestingly, “Vision Disabled” models significantly outperform chance on object size tasks, likely due to the integration of common-sense knowledge from language model training.
            </p>

            <d-figure id="fig-blind-eval" >
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/blind_evaluation.png" alt="benchmark category" style="width: 70%; display: block; margin: 0 auto;">
                    <figcaption>
                        <strong>Figure 5</strong>: Performance comparisons between Vision Enabled
                        (w/ video), Vision Disabled (w/o video) and Chance Level
                        (Freq.). 
                    </figcaption>
                </figure>
            </d-figure>

            <div id='visual-representation' class="viusal-representation-block">
            <h1 class="text">How MLLMs Think in Space Linguistically</h1>
                
                <p class="text">
                    To better understand when and why models succeed or fail and to elucidate the facets of visual-spatial intelligence they possess, we examine how MLLMs think in space linguistically.
                </p>

                              
                </d-figure>
                <p class="text">
                    <strong>Case Studies</strong>:
                    In the success example, the model demonstrates advanced video understanding with accurate timestamped descriptions and a correct step-by-step reasoning process. The use of a global coordinate system suggests that MLLMs may construct implicit world models by integrating spatial context and reasoning. In the error case, the model fails in egocentric-allocentric transformation, incorrectly interpreting a video sequence due to reliance on the egocentric view, leading to a flawed spatial inference.
                </p>

                <d-figure id="fig-case-study" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/case-study.png" alt="benchmark category" style="width: 90%; display: block; margin: 0 auto;">
                        <figcaption>
                            <strong>Figure 6</strong>: Examples of how a MLLM thinks as seen in self-explanations. 
                        </figcaption>
                    </figure>
                </d-figure>

            
                <p class="text">
                    <strong>Error Analysis</strong>:
                    Analysis of errors from the best-performing MLLM on VSI-Bench (tiny) identifies four main error types: visual perception, linguistic intelligence, relational reasoning, and egocentric-allocentric transformation. Figure 6 reveals that 71% of errors stem from spatial reasoning, particularly in understanding distance, size, and direction. This indicates that spatial reasoning remains the key bottleneck for improving MLLM performance on VSI-Bench.
                </p>
                <d-figure id="fig-error-breakdown" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/error_breakdown.png" alt="benchmark category" style="width: 60%; display: block; margin: 0 auto;">
                        <figcaption>
                            <strong>Figure 7</strong>: Human-conducted analysis of errors by type. 
                        </figcaption>
                    </figure>
                </d-figure>

                <div class="highlight-box">
                    <em>Finding 1: Spatial reasoning is the primary bottleneck for MLLM performance on VSI-Bench.</em>
                </div>



                
                
            </div>

                <p class="text">
        
                    <strong>Limits of CoT Methods in Visuospatial Tasks</strong>:
                    We investigate three prompting techniques—Zero-Shot Chain-of-Thought (CoT), Self-Consistency with CoT, and Tree-of-Thoughts (ToT)—to improve MLLM reasoning on VSI-Bench. 
                    Surprisingly, all three methods led to performance degradation (see Fig. 8), 
                    with Zero-Shot CoT and ToT reducing average performance by 4%, 
                    and Self-Consistency falling 1.1% below the baseline. 
                    While the appearance order and absolute distance estimation tasks saw slight improvements due to reduced linguistic errors, 
                    the room size and object size tasks suffer a large 8% to 21% decrease, 
                    showing that encouraging a model to think more can be not just unreliable
                    but downright harmful.
                </p>
                <d-figure id="fig-zero-shot" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/zero-shot.png" alt="benchmark category" style="width: 60%; display: block; margin: 0 auto;">
                        <figcaption>
                            <strong>Figure 8</strong>: Relative improvements of CoT, self-consistency and
                            Tree-of-Thought compared to the baseline. 
                        </figcaption>
                    </figure>
                </d-figure>
                <p class="text">
                    Meanwhile, as shown in Tab. 2, ZeroShot CoT achieves a 1.6% improvement on the general
                    video understanding benchmark VideoMME.
                </p>
            
                <div id="tab:data_balance_result" style="display: flex; flex-direction: column; align-items: center;">
                    <div class="table-container">
                        <table class="data-table">
                        <thead>
                            <tr>
                                <th>Case</th>
                                <th>Performance</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Gemini-1.5 Pro (w/o CoT)</td>
                                <td>77.2</td>
                            </tr>
                            <tr>
                                <td>Gemini-1.5 Pro (w/ CoT)</td>
                                <td>79.8</td>
                            </tr>
                        </tbody>
                    </table>

                </div>
                <figcaption style="text-align: center; width: 100%;">
                    <strong>Table 2:</strong> Gemini-1.5 Pro CoT performance on a 500-questions
                    subset in VideoMME.
                </figcaption>
                <div class="highlight-box">
                    <em>Finding 2: Linguistic prompting techniques, although effective in language reasoning and general visual tasks, are primarily harmful for spatial reasoning.</em>
                </div>
                
            </div>        
        </div>

           
        </div>
            <div id='vsi-world' class="vsi-world">

                <div id='vsi-improvements' class="vsi-improvements">



                <h1 class="text"> How MLLMs Think in Space Visually</h1>
                <p class="text">
                    Since humans subconsciously build mental representations of space when reasoning spatially, we explore how MLLMs remember spaces. </p>

                    <p class="text">
                        <strong> Probing via Cognitive Maps</strong>:
                        We evaluate MLLMs’ ability to create cognitive maps, a framework for spatial representation, by prompting Gemini-1.5 Pro to predict object center positions within a 10 × 10 grid based on video input. Accuracy was measured by comparing predicted object distances with ground truth maps, considering deviations within one grid unit as correct. The model achieved 64% accuracy in positioning adjacent objects, demonstrating strong local spatial awareness but struggling with larger distances, reflecting challenges in forming global spatial representations from discrete video frames. 
                    </p>

                    <d-figure id="fig-cog-map" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/cog-map.png" alt="benchmark category" style="width: 90%; display: block; margin: 0 auto;">
                            <figcaption>
                                <strong>Figure 9</strong>. <strong>Left</strong>: Visualizations of cognitive maps from MLLM and GT. <strong>Right</strong>: Locality of the MLLM’s predicted cognitive maps.
                            </figcaption>
                        </figure>
                    </d-figure>
                    <div class="highlight-box">
                        <em>Finding 3: When remembering spaces, a MLLM forms a series of local world models in its mind from a given video, rather than a unified global model.</em>
                    </div>

                    <p class="text">
                        <strong>Better Distance Reasoning via Cognitive Maps</strong>:
                        We explored whether cognitive maps could enhance MLLMs’ spatial reasoning by prompting Gemini-1.5 Pro to generate a map from video input and use it to answer relative distance questions. Results showed a 10% accuracy improvement with the model’s own map and a 20%-32% gain using ground truth maps, highlighting the value of accurate mental imagery for enforcing global scene topology. This suggests cognitive mapping as a promising approach to improve MLLMs’ visual-spatial reasoning. 
                    </p>
                    <div style="display: flex; justify-content: center; align-items: flex-start; gap: 20px;">
                        <!-- Table (a) -->
                        <table border="1" style="border-collapse: collapse; text-align: center;">
                            <caption style="caption-side: bottom; font-style: italic;">(a) Cognitive map prompting.</caption>
                            <thead>
                                <tr>
                                    <th>Case</th>
                                    <th>Rel. Dist Acc.</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>w/o Cog. map</td>
                                    <td>46.0</td>
                                </tr>
                                <tr>
                                    <td>w/ Cog. map</td>
                                    <td>56.0</td>
                                </tr>
                                <tr>
                                    <td>w/ Cog. map (GT)</td>
                                    <td>66.0</td>
                                </tr>
                            </tbody>
                        </table>
                    
                        <!-- Table (b) -->
                        <table border="1" style="border-collapse: collapse; text-align: center;">
                            <caption style="caption-side: bottom; font-style: italic;">(b) Cognitive map canvas size.</caption>
                            <thead>
                                <tr>
                                    <th>Cog. Map Src.</th>
                                    <th>Size</th>
                                    <th>Rel. Dist Acc.</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>MLLM</td>
                                    <td>10 × 10</td>
                                    <td>56.0</td>
                                </tr>
                                <tr>
                                    <td>MLLM</td>
                                    <td>20 × 20</td>
                                    <td>54.0</td>
                                </tr>
                                <tr>
                                    <td>GT</td>
                                    <td>10 × 10</td>
                                    <td>66.0</td>
                                </tr>
                                <tr>
                                    <td>GT</td>
                                    <td>20 × 20</td>
                                    <td>78.0</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    
                    <!-- Combined Table Caption -->
                    
                    <figcaption style="text-align: center; width: 100%;">
                        <strong>Table 3:</strong> Relative distance task with cognitive map.</p>
                    </figcaption>
                </div>

        </div>
        </div>
    </div>

       

        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                We study how models see, remember, and recall spaces by building VSI-Bench and investigating the performance 
                and behavior of MLLMs on it. Our analysis of how MLLMs think in space linguistically and visually identifies existing strengths (e.g., prominent perceptual, temporal, and linguistic abilities) and bottlenecks for visual-spatial intelligence (e.g., egocentric-allocentric transformation and relational reasoning). While prevailing linguistic prompting methods fail to improve spatial reasoning, building explicit cognitive maps does enhance the spatial distance reasoning of MLLMs.

            </p>
        </div>
    </div>

        </d-article>
        
        <d-appendix>

            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{yang2024think,<br>
                    &nbsp;&nbsp;title={{Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces}},<br>
                    &nbsp;&nbsp;author={Yang, Jihan and Yang, Shusheng and Gupta, Anjali and Han, Rilyn and Fei-Fei, Li and Xie, Saining},<br>
                    &nbsp;&nbsp;year={2024},<br>
                    &nbsp;&nbsp;journal={arXiv preprint},<br>
                }
            </p>
            
            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>  
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="/static/js/nav-bar.js"></script>

    </body>
</html>
